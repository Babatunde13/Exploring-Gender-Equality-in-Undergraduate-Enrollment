---
title: "EM Algorithm (Mixture of Univariate Gaussian)"
author: "Kehinde Fagbamigbe"
date: '2022-08-08'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Expectation-Maximization Algorithm (Mixture of Univariate Gaussian)
# E-Step
```{r}
E.step <- function(x, tau, Mu, S2){  #tau is mixture proportion, Mu is mean S2 is standard deviation

 K <- length(tau)
 n <- length(x)

 Pi <- matrix(NA, n, K)

 for (i in 1:n){
  for (k in 1:K){
   Pi[i,k] <- tau[k] * dnorm(x[i], Mu[k], sqrt(S2[k]))  #dnorm means normal distribution #for each observation get the posterior pr for each cluster
  } 
  Pi[i,] <- Pi[i,] / sum(Pi[i,])
 }

 return(Pi)

}
```




```{r}
E.step(8,2,0.5,4)
```


```{r}
matrix(NA, 2, 4)

```


# M-Step
```{r}
M.step <- function(x, Pi){

 K <- dim(Pi)[2]
 n <- dim(Pi)[1]
 
 Sum.Pi <- apply(Pi, 2, sum)

 tau <- Sum.Pi / n

 Mu <- rep(0, K)
 S2 <- rep(0, K)

 for (k in 1:K){
 
  for (i in 1:n){
   Mu[k] <- Mu[k] + Pi[i,k] * x[i]
  }
  Mu[k] <- Mu[k] / Sum.Pi[k]

  for (i in 1:n){   
   S2[k] <- S2[k] + Pi[i,k] * (x[i] - Mu[k])^2
  }
  S2[k] <- S2[k] / Sum.Pi[k]

 }

 return(list(tau = tau, Mu = Mu, S2 = S2))

}
```


```{r}
#M.step(8,45)
```



```{r}
logL <- function(x, tau, Mu, S2){

 n <- length(x)
 K <- length(tau)

 ll <- 0

 for (i in 1:n){

  ll2 <- 0

  for (k in 1:K){
   ll2 <- ll2 + tau[k] * dnorm(x[i], Mu[k], sqrt(S2[k])) 
  }

  ll <- ll + log(ll2)

 }

 return(ll)

}




EM <- function(x, tau, Mu, S2, eps){

 n <- length(x)
 K <- length(tau)

 b <- 0

 ll.old <- -Inf
 ll <- logL(x, tau, Mu, S2)

 # cat("Iteration", b, "logL =", ll, "\n")
 
 repeat{

  b <- b + 1

  if ((ll - ll.old) / abs(ll) < eps) break

  ll.old <- ll

  Pi <- E.step(x, tau, Mu, S2)

  M <- M.step(x, Pi)
  tau <- M$tau
  Mu <- M$Mu
  S2 <- M$S2

  ll <- logL(x, tau, Mu, S2)

  # cat("Iteration", b, "logL =", ll, "\n")

 }

 id <- apply(Pi, 1, which.max)

 M <- 3 * K - 1
 BIC <- -2 * ll + M * log(n)
 AIC <- -2 * ll + M * 2

 return(list(tau = tau, Mu = Mu, S2 = S2, Pi = Pi, id = id,
  logL = ll, BIC = BIC, AIC = AIC))

}
```


```{r}
tau <- c(0.2, 0.5, 0.3)
Mu <- c(-2, 2, 5)
S2 <- c(1, 0.5, 2)

K <- length(tau)
n <- 1000

nk <- rmultinom(1, n, tau) #The Multinomial Distribution
```


```{r}
x <- NULL
for (k in 1:K){
 x <- c(x, rnorm(nk[k], Mu[k], sqrt(S2[k]))) #The Normal Distribution
}

hist(x, freq = FALSE)

tau.0 <- rep(1/3, 3)
Mu.0 <- c(-1, 0, 1)
S2.0 <- c(1, 1, 1)

A <- EM(x, tau = tau.0, Mu = Mu.0, S2 = S2.0, eps = 1e-8)

t <- seq(-5, 10, by = 0.01)

d <- rep(0, length(t))
for (k in 1:K){
 d <- d + A$tau[k] * dnorm(t, A$Mu[k], sqrt(A$S2[k]))
}

points(t, d, type = "l")


# K = 2

tau.0 <- rep(1/2, 2)
Mu.0 <- c(-2, 1)
S2.0 <- c(1, 1)

A2 <- EM(x, tau = tau.0, Mu = Mu.0, S2 = S2.0, eps = 1e-8)
A2$logL
A2$BIC

# K = 3

tau.0 <- rep(1/3, 3)
Mu.0 <- c(-1, 0, 1)
S2.0 <- c(1, 1, 1)

A3 <- EM(x, tau = tau.0, Mu = Mu.0, S2 = S2.0, eps = 1e-8)
A3$logL
A3$BIC

# K = 4

tau.0 <- rep(1/4, 4)
Mu.0 <- c(-2, -1, 0, 1)
S2.0 <- c(1, 1, 1, 1)

A4 <- EM(x, tau = tau.0, Mu = Mu.0, S2 = S2.0, eps = 1e-8)
A4$logL
A4$BIC


# K = 5

tau.0 <- rep(1/5, 5)
Mu.0 <- c(-2, -1, 0, 1, 2)
S2.0 <- c(1, 1, 1, 1, 1)

A5 <- EM(x, tau = tau.0, Mu = Mu.0, S2 = S2.0, eps = 1e-8)
A5$logL
A5$BIC





```

